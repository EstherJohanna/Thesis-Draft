\chapter{Statistical Analysis} \label{Chapter3} 


\section{Meta-analysis of literature by regression}\label{regLit}

Several studies have investigated on factors that influence takeover time. While it is impossible to study all influencing factors in one study, it is possible to compare the studies' results with each other. The aim of the following meta-analysis was to find across studies which influencing factors are the most crucial ones to consider in a takeover model. In addition, a regression equation that includes all influencing factors can be used to predict takeover time. 

\subsection{Literature search}
We found a broad width of studies through an existing database at Bosch. Furthermore, we chose papers with matching titles from a Google scholar search with the keywords `takeover time highly automated driving'. From both searches, further publications were found through the cited references. A sum of \~{} 100 papers was found. 

\subsection{Selection of Studies}
Of those papers, all papers were chosen that were concerned with Highly Automated Driving (SAE Level 3). In addition, we had the following inclusion criteria:
\begin{itemize}
\item The subjects knew that a takeover would occur and were instructed on how to behave in that case (Excluded papers: ...);
\item Takeover consists of a takeover request, time given for the takeover, and a full transfer of control after the driver indicates that he wants to take back control (Excluded papers: );
\item studies that measured takeover time (excluded ..., because they gave 8 seconds time and measured how many mistakes were made)
\item enough information about the study was given (Excluded : ...);
\item they measured something that we expect to have an influence `on the road'; excluded ... because they test visual vs. visual-auditory takeover requests; we expect always visual-auditory to be used;
\item about highly automated driving in a car, not platooning (excluded ...);
\item where did they find a difference between the conditions; excluded ... because they showed that presenting the RtI on a cell phone vs. a display does not make a significant difference.
\end{itemize}
By this, we have a selection of 19 papers to be compared in the meta-analysis. Those were \cite{unfallforschung, eriksson2016take, korber2016influence, gold2016taking, maas2016losungsansatze, zeeb2016take, bueno2016different, louw2015engaging, feldhutter2016duration, befelein2016, Gold2015, Zeeb2015, kerschbaum2015transforming, walch2015autonomous, Lorenz2014, radlmayr2014traffic, Gold2013, feldhuetterFahrer, Gold2016}.

\subsection{Criteria for Selection of Influencing Factors}
As influencing factors we chose all factors that showed to have a relevant influence in the 19 papers. In addition, we chose two factors that we considered to be important and that could be inferred from the papers. 
\\
The factors chosen from the papers were the following:
\begin{itemize}
\item Gaze Off Road
\item Body Turned Away
\item Hands Occupied
\item Cognitive Distraction
\item Much Traffic
\item Criticality Of Situation
\item Time Pressure
\item Long Autonomous Ride
\item Elderly Subject
\end{itemize}

The factors chosen because of their relevance were:
\begin{itemize}
\item Mirror Check Needed
\item Decision Necessary
\end{itemize}

\subsection{Regression Equation}\label{equ}
Studies report mean or median, and some standard deviation. Data on single subjects is not given. Therefore, one line in design- and result matrix does not (as usual) include one subject, but one condition in a study. For example, if \cite{unfallforschung} had one baseline condition that on average took 2,2 seconds for the takeover, the first number of the result vector was 2,2. For the Design matrix, we filled out every condition as `True', `Not True', or `Not Measured'. As an example, I show how the row of the design matrix for\cite{unfallforschung}'s baseline condition was filled out:
 Gaze Off Road (`Not True'), Mirror Check Necessary(`Not Measured'), Body Turned Away(`Not True'), Hands Occupied(`Not True'), Cognitive Distraction(`Not True'), Much Traffic(`Not Measured'), Criticality of Situation(`Not Measured'), Decision Necessary(`Not Measured'), Time Pressure(`True'), Long Autonomous Ride(`Not True'), Elderly Subject(`Not Measured'). From a `regression' point of view, it would have been ideal to have a series of homogeneous observations with identical boundary conditions that test all possible combinations of factors. This will be assumed for the rest of the analysis and should be kept in mind as a possible source of error for the discussion. 
 \\
 A complete version of design- and result matrix can be found in \ref{Appendix}. 
\\
While some of the factors are `naturally binary' (Gaze Off Road, Mirror Check Necessary, Body Turned, Hands Occupied and Decision Necessary), for others a threshold needed to be set. Elderly Subject, Long Autonomous Ride and Tiredness were binary because they were only investigated in one study each with a relevant difference. Only one study included subjects over 60 years old [\cite{korber2016influence}]; and only one study drives 20 minutes highly automated until they request to take over [\cite{feldhutter2016duration}]. Cognitive Distraction was rated as `High' when the subject played tetris, solved the 20 question task, read an email, solved the difficult remote association test, calculated, did the 2-back task, did a cognitive-motoric task or had to do a fill-in-the-blanks text. It was rated low when the subjects did nothing, read newspaper, watched a video, solved the easy remote association task or the surrogate reference task. Reading emails was assumed to be cognitively more demanding than reading newspapers, because one has to come up with an answer to emails, but not to the newspaper. In \cite{feldhuetterFahrer} subjects could choose their activities which was rated as cognitively demanding, because we assume subjects to choose something that keeps them busy and not bored. For traffic, the threshold was chosen between no traffic at all and traffic existed on the road. This showed to induce a bigger difference than little and much traffic in \cite{gold2016taking}. A takeover situation was rated urgent when a collision had to be avoided, and as uncritical when a ride simply had to be continued on a straight road. Also `curve with wind gust' in \cite{Zeeb2015} was rated as urgent. For Time Pressure the threshold was set at 6 seconds, because Gold et al. find that at 5 seconds an overhasty takeover occurs, but not at 7 seconds [\cite{Gold2013}]. 
\\
Thus, we postulate the following regression equation:
\begin{equation}
\begin{split}
TakeOverTime = \beta_{0} + \beta_{1} \cdot Gaze Off Road + \beta_{2} \cdot Mirror Check Necessary + \\
\beta_{3} \cdot Body Turned Away + \beta_{4} \cdot Hands Occupied  + \beta_{5} \cdot Cognitive Distraction + \\
\beta_{6} \cdot Much Traffic + \beta_{7} \cdot Criticality of Situation +  \beta_{8} \cdot Decision Necessary + \\
\beta_{9} \cdot  Time Pressure + \beta_{10} \cdot Long Autonomous Ride  + \beta_{11} \cdot Elderly Subject  + \\ 
\epsilon_{i}
 \end{split}
\end{equation} 

If participants have their gaze off road during the takeover request, it is expected that they take longer for the takeover because additional time is needed to direct the gaze towards the road. The same holds for a necessary mirror check, a turned away body , occupied hands, cognitive distraction and a decision that needs to be taken. If there is much traffic, we expect that it takes longer to build up situation awareness before the takeover, because the situation is more complex. The Criticality of the situation is supposed to speed up the takeover in case of an urgent situation (e.g. collision avoidance) and to slow down in the case of a not urgent scenario (e.g. simply continue to drive on a straight road). Under higher time pressure, the subject is also expected to take over faster than without time pressure. After a long autonomous ride, boredom and tiredness are expected to make the whole takeover slower. The effect of age is expected to be small, according to studies. 

\subsection{Checking the Multiple Regression Requirements}
This has been done after the manual on \cite{statsZuerich}.
\\
\subsubsection{Linearity of the problem}\label{req1}
Since we postulated a linear model, we have to check whether all independent variables have a linear relationship with the dependent variable when controlled for the other factors. This can be done by looking at the partial regression diagrams (See Fig \ref{regdiag} ). On the Y axis this shows the residual that would result if the dependent variable would be regressed on all independent variables except the one written on X. On the X axis it shows the residual that would result if X would be regressed on all other variables.  Thus, the part of the variance that is not explained by all factors but X is plotted against the part of X that is independent of the other variables. If these scatterplots look linear, we can assume a linear relationship. We will neglect for simplicity of analysis that most of the scatter plots do not look linear. 


\begin{figure}
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Figures/4}
        \caption{Gaze Offroad}
        \label{fig:gull}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Figures/5}
        \caption{Mirror Check Necessary}
        \label{fig:tiger}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Figures/6}
        \caption{Body Turned Away}
        \label{fig:mouse}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Figures/7}
        \caption{Hands Occupied}
        \label{fig:gull}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Figures/8}
        \caption{Cognitive Distraction}
        \label{fig:tiger}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Figures/9}
        \caption{Traffic}
        \label{fig:mouse}
    \end{subfigure}
        \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Figures/10}
        \caption{Criticality of Situation}
        \label{fig:gull}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Figures/11}
        \caption{Decision Necessary}
        \label{fig:tiger}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Figures/12}
        \caption{Time Pressure}
        \label{fig:mouse}
    \end{subfigure}
        \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Figures/13}
        \caption{Long Autonomous Ride}
        \label{fig:gull}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Figures/14}
        \caption{Elderly Subject}
        \label{fig:tiger}
    \end{subfigure}
    \caption{Partial Regression Diagrams}\label{regdiag}
\end{figure}


\subsubsection{Linearity of the coefficients (Gauss-Markov-Theorem 1)}
We assume that the postulated model is linear in the coefficients: 

\begin{equation}
\begin{split}
TakeOverTime = \beta_{0} + \beta_{1} \cdot Gaze Off Road + \beta_{2} \cdot Mirror Check Necessary + \\
\beta_{3} \cdot Body Turned Away + \beta_{4} \cdot Hands Occupied  + \beta_{5} \cdot Cognitive Distraction + \\
\beta_{6} \cdot Much Traffic + \beta_{7} \cdot Criticality of Situation +  \beta_{8} \cdot Decision Necessary + \\
\beta_{9} \cdot  Time Pressure + \beta_{10} \cdot Long Autonomous Ride  + \beta_{11} \cdot Elderly Subject  + \\ 
\epsilon_{i}
 \end{split}
\end{equation}

\subsubsection{Random Sample (Gauss-Markov-Theorem 2)}\label{req3}
We assume that all subjects were drawn from a random sample. Most likely we have a skew for a WEIRD sample (western, educated, industrialized, rich, democratic, see \cite{jones2010weird}) , which needs to be kept in mind. 

\subsubsection{Strict Exogeneity (Gauss-Markov-Theorem 3)}\label{req4}
This hypothesizes that the the error term has a mean of zero for every value of an independent variable. For this, we look at a scatter plot of the standardized estimated value of y on the X-axis, and the standardized error terms (residuals) on the Y-axis, see Fig. \ref{1} . By visual inspection we estimate that the mean error is zero.

\begin{figure}
\centering
\includegraphics[width = 0.5\textwidth]{Figures/1}
\decoRule
\caption{Scatter Plot of errors.}
\label{1}
\end{figure}

%prev
\subsubsection{Homoscedasticity}
This means that the error has the same amount of variance for every value of the independent variable. This can also be checked by visual inspection of Fig. \ref{1}. Now, it is important that no `shape' can be seen. The dots need to be equally distributed along the x-axis. This seems to be the case. This can be statistically proven by e.g. the Breusch-Pagan-Test or the White-test , which would go beyond the scope of this thesis.

\subsubsection{Independence of the Error Term}\label{req5}
This requires that the errors of the independent variables are independent (thus, have no influence on each other). This can be checked by the Durbin-Watson test. The statistic can have values between 0 and 4, both of which indicate a high correlation, while 2 indicates a low correlation. In our case, the Durbin-Watson value is 0.99, which indicates medium correlation between the error terms. 

\subsubsection{Normal Distribution of the Error Term}
The residuals should be normally distributed. This can be checked and confirmed by Fig. \ref{2}

\begin{figure}
\centering
\includegraphics[width = 0.5\textwidth]{Figures/2}
\decoRule
\caption{Histogram of residuals.}
\label{2}
\end{figure}

\subsubsection{No Multicollinearity}\label{multi}
Table \ref{frequ} shows a frequency table of the design matrix. Just by looking at the frequency table, it becomes obvious that some factors are varied only very little. Besides this being a problem because of little information available, it is also a problem because all factors that are kept the same over most studies correlate highly with the constant, which is a row of ones in front of the design matrix. In general, we have the problem that many factors correlate. This can be seen in Fig. \ref{bla} . While correlations between Takeover time (`Uebernahmezeit') and all the other factors (first row) are desired, all other correlations between factors are not. Here it has to be kept in mind that all correlations between factors only indicate how often they were measured together in studies, and says nothing about a relationship between the factors themselves. A correlation between any factor with takeover time on the other hand does say something about the influence of this factor on takeover time.`Hands Occupied' (`Handbelegung') and `Gaze Off Road' have a Pearson-Correlation of 0.931. This makes sense because most likely one looks towards one's hands if those are occupied. Time pressure (`Zeitnot') and Mirror Check (`Spiegelblick') have a correlation of 0.71, which might be coincidence. Also other factors have more or less high correlations. In other words, variables party explain each other and do not necessarily add completely new information. This can be compared to a matrix that does not have full rank because its rows are linearly dependent; it results in an ill-defined problem because we have more variables we want to find than equations that can independently explain those variables. Thus, using a design matrix with factors that correlate leads to imprecise estimates of the regression parameters. Unfortunately, it can not be clearly said how much correlation is too much correlation. For this, the measures of Tolerance Value and Variation Inflation Factor (`VIF', which is simply the inverse of the Tolerance Value) are typically examined. Those are measures of multicollinearity. While one can have multiple variables that do not correlate pairwise, there might still be significant multicollinearity between the set of all of them. The Tolerance Value is calculated by $T_{j} = 1- R^{2}$, where $R^{2}$ is the coefficient of Determination. $R^{2}$ measures how much of the variance of the dependent variable is explained by this independent variable. In other words, VIF and Tolerance Value measure how much larger the variance of the regression coefficient is than if the variable had been completely uncorrelated with other variables in the model. This Tolerance Value should, as a rule of thumb, not be smaller than 0.1. Including correlating variables in a model leads to p-values that are higher (=less significant) by a factor of the VIF than they would otherwise would be. 

\begin{center}
  \begin{tabular}{ l | c | c | c| r}
    \hline
    Factor & Amount 0 & Amount 1 & Amount -1 & Sum\\ \hline
    Gaze Off Road & 9 & 51 & 53 & 113\\ \hline
    Mirror Check Necessary & 6 &41 &66 &113\\ \hline
    Body Turned Away & 7 & 4 & 102 & 113\\ \hline
    Hands Occupied & 6 & 56 &51 &113\\ \hline
    Cognitive Distraction & 25 & 28 &  60 & 113 \\ \hline
     Much Traffic & 15 & 32 & 66 & 113\\ \hline
     Criticality of Situation & 6 &  100 & 7 & 113 \\ \hline
     Decision Necessary & 6 & 23 & 84 &  113 \\ \hline
     Time Pressure & 5 &  38 & 70 & 113 \\ \hline
      Long Autonomous Ride & 11 & 3 & 99 & 113\\ \hline
      Elderly Subject & 77 & 21 & 15 & 113\\ \hline
    \hline
    \label{frequ}
  \end{tabular}
\end{center}

\begin{figure}
\centering
\includegraphics[width = 1\textwidth]{Figures/Korrs}
\decoRule
\caption{Correlations between all variables.}
\label{bla}
\end{figure}


\begin{figure}
\centering
\includegraphics[width = 1\textwidth]{Figures/3}
\decoRule
\caption{Coefficients and test for multicollinearity.}
\label{3}
\end{figure}

As can be seen in Fig. \ref{3}, none of the Tolerance Values (`Toleranz', second-last column under `Kollinearitaetsstatistik') are smaller than 0.1. While this first seems surprising due to the correlation between several factors, Belsley writes in his book on `Regression Diagnostics' that high correlations are not always problematic [\cite{belsley2005regression}]. Some authors even suggest correlations as high as 0.7 should not be considered severe [\cite{mason1991collinearity}]. Thus, we will continue our analysis while keeping the high correlations in mind for subsequent analysis. They are one reason we will try a singular value decomposition in part \ref{svd}.\\

\subsection{Results}\label{resLit}
In Fig. \ref{3} it can be seen that the constant (`Konstante'), much traffic (`viel Verkehr') and criticality of the situation (`Dringlichkeit Situation') are significant (sixth column `Sig.'). This significance depends on 1) the magnitude of the effect, 2) the magnitude of the error variance, 3) the variance of the variable itself, 4) the amount of data, and 5) the number of variables in the model. All five can be reasons that the other variables did not become significant. Insignificant variables need to be excluded in the regression equation. Thus, our resulting regression equation looks the following:

\begin{equation}
\begin{split}
TakeOverTime = 3.16 + 0.46 \cdot Much Traffic - 0.43 \cdot Criticality of Situation + \epsilon_{i}
 \end{split}
\end{equation}

`Much traffic' and `Criticality of Situation' have an effect of similar strength, but in opposite directions. It makes sense that a person takes longer to prepare for the takeover if there is more traffic, and that one is faster if the situation is critical. The constant of 3.15 seconds suits to typical takeover times in literature. Those two factors having the biggest influence suits to the recently published dissertation by Christian Gold. He finds by regression analysis that Time Budget, Repetition (which was not included in our model) and traffic density are the main predictors of takeover time [\cite{goldmodeling}]. The insignificant variables should not be interpreted due to their insignificance and correlation with one another. Calculating the same regression in Matlab and Excel confirmed our results. 
\\
Also the Goodness of fit ($R^2$) of the whole model can be measured with SPSS (Fig. \ref{15}, upper part `Modelluebersicht'). As already mentioned, $R^2$ measures the amount of variability in takeover time that is explained by the model. The adjusted $R^2$ is corrected by the amount of independent variables. This is necessary because $R^2$ rises with the amount of independent variables, even if they do not add any information. The adjusted $R^2$ is reduced by the number of independent variables and increased by sample size. The archievable $R^2$ depends on the variance present in the data. Thus, also a model with a low  $R^2$ can be a good model, if there is a lot of `natural' variance in the data (like e.g. with humans) [\cite{goldmodeling}]. In our case, 22,7\% of the variability of takeover time are explained by our eleven factors. A plot of the predicted against the measured takeover time can be found in Fig. \ref{16}. The significance of different studies can be compared by the effect size. The resulting effect size can be calculated by Cohen's d: $\sqrt{\frac{0.227}{1-0.227}} = 0.53$. This corresponds to a strong effect according to Cohen [\cite{cohen1992power}]. 
The lower part of Fig. \ref{15} shows the ANOVA of the complete regression model, which is calculated to be significant. 

\begin{figure}
\centering
\includegraphics[width = 1\textwidth]{Figures/15}
\decoRule
\caption{($R^2$) and ANOVA of the whole model.}
\label{15}
\end{figure}

\begin{figure}
\centering
\includegraphics[width = 0.5\textwidth]{Figures/16}
\decoRule
\caption{X-Axis: Predicted Value of takeover time; Y-Axis: measured Value of takeover time.}
\label{16}
\end{figure}

\subsection{Discussion}
This approach has used the `enter' method for including variables in the regression. When using the stepwise forward regression not all variables are in included at once, but variable per variable is included until adding another makes no significant change in the model anymore. 
The question whether it is better to include all variables that one thinks to have an influence, or to do so stepwise until no further improvement of significance is reached divides opinions. A criticism of the stepwise regression is that one `picks out' the most suitable variables. Typically, for testing an a-priori hypothesis, the `enter' method is used (thus, all variables are being included). I assume that all factors have a relevant influence, and with my regression I only want to show which factor has which weighting. The stepwise regression is used for more explorative approaches: I am not sure whether all eleven factors have an influence, so I test which ones make a significant difference. Considering that I chose all factors from literature that showed that the corresponding factor does have an influence, the `enter' method seems more suitable. 


\begin{figure}
\centering
\includegraphics[width = 1\textwidth]{Figures/17}
\decoRule
\caption{Result of the regression with non-correlating factors and concordant studies only.}
\label{17}
\end{figure}

\begin{figure}
\centering
\includegraphics[width = 0.5\textwidth]{Figures/18}
\decoRule
\caption{Scatter plot of the regression with non-correlating factors and concordant studies only. The X-axis shows the predicted value, the Y-axis the measured value of takeover time.}
\label{18}
\end{figure}


When testing the multiple regression requirements, all seem to be fulfilled except for the linearity of the problem (Fig. \ref{regdiag}). Even though the VIF indicates that no multicollinearity exists, a high correlation between variables might still have an influence and lead to high p-values and high variability of estimated betas. There certainly is another problem hidden in the design matrix: some studies that investigated the same factor conclude conflictive results. While for example most studies find that a gaze off road leads to longer takeover times, Louw et al. have a lower mean takeover time in the group that had the gaze off road [\cite{louw2015engaging}]. Results after removing all studies that have conflictive results, and after removing all factors that show high correlation with another factor, can be found in Fig. \ref{17}. This regression has an adjusted $R^2$ of 11.8\%, see Fig. \ref{18}. Another problem might be that studies are less comparable than expected. Most likely, factors other than the ones included in our regression varied between studies. This might be little things: Differences in simulators, study setup, study subjects, etc. might be reasons that make studies incomparable. Also, a nominal scale has a low level of measurement. This can hardly be changed because most measures are nominal by how they were measured or reported in the given studies. Furthermore, in many studies takeover times were reported independent of takeover quality. Thus, if an overhasty takeover leading to high lane deviations and lateral accelerations occurred, takeover time was very short. Even if e.g. traffic would have had an influence on the takeover time, this would have been omitted by the fact that the takeover happened overhasty. Since we are modeling a normative (`driving school') takeover, we would be interested in any factor that influences takeover time in the case of a calm, planned takeover. In the end, it has to be taken into account that it might be that we simply included several independent variables that have no or very little explanatory value. Those would then compete for explanatory power (especially if they are correlated) and thus decrease all p-values.

\subsection{Outlook}
Suggestions to improve the regression equation would be to try nonlinear factors (in a still linear equation). Furthermore, a nonlinear regression equation could be tried. In case we chose a linear regression even though the correct model is not linear, the slope and intercept estimates and the fitted values from the linear regression will be biased [\cite{statsrequ}]. Due to several advantages when comparing studies, we propose to use a random-effect meta-regression. Other than a linear regression, the meta-regression takes into account the sampling variances of the observed outcomes or effect size estimates.   

\section{Singular Value Decomposition} \label{svd}
\subsection{Method}
\begin{figure}
\centering
\includegraphics[width = 0.3\textwidth]{Figures/19}
\decoRule
\caption{Conceptual idea of the singular value decomposition}
\label{19}
\end{figure}
Another option to interpret the (previously called) design- and result matrix is Singular Value Decomposition (`SVD'). Its advantage is that it is not disturbed by multicollinearity in the design matrix, because it clusters variables that are collinear. While SVD can not be used to predict takeover time, it can be used to reduce the dimensionality of the problem (thus, the amount of influencing factors). This might be useful for other modeling approaches or future study design.  
A more intuitive example to explain singular value decomposition is to explain principle component analysis (`PCA'), of which the core part is the SVD. In PCA, one tries to explain observations for many particular things in few general explanations. For example, if I have 100 CDs at home, when asked for my music preference, I will most likely not answer all 100 names of those CDs, but `I like Jazz'. Another example is handling the data of Magnetic Resonance Imaging: Instead of reporting all voxels of all subjects that were active for a certain task, I report `For all subjects, the motor cortex was active'. PCA tries to find those underlying features like `Jazz' and `motor cortex' in our examples. Graphically, this can be imagined as shown in Fig. \ref{19}. While factors 1 to n correspond to the many voxels with different activities, their change in activity can be traced back to a combination of activity in `motor cortex' and `visual cortex' , which are the `real' explanatory factors behind the differences in the result (h1 and h2). In mathematical terms, this is called dimensionality reduction. As an example, we will reduce a 3-dimensional matrix F to a 2-dimensional matrix. We plot the matrix in 3-D space and draw a 2-D plane into it. Subsequently, we project all data points on this 2-D plane. This projection is now our matrix with reduced dimensions. The plane should be drawn in a way that the projected distance is as short as possible. To do this by singular value decomposition, the first step is to perform a mean normalization and feature scaling on matrix F with size (n x p). Second, the covariance matrix $\Sigma$ is calculated for F. Subsequently, the singular value decomposition is computed on $\Sigma$ to obtain its eigenvectors. 

As an equation:
\begin{equation}
\begin{split}
[U, S, V] = SVD(\Sigma).
 \end{split}
\end{equation}

Now, U (n x p) are the eigenvectors of $\Sigma$ , which corresponds to the hyperplane in 2D in our example. S (p x p) is the diagonal matrix containing the singular values. They give the amount of variance retained by the reduction. V is an (p x p) matrix that transforms all correlated variables into uncorrelated ones. 
Now, we are interested in the influence of these singular values on our result vector r (n x 1). It contains n takeover times in our case. Thus, we calculate:

\begin{equation}
\begin{split}
\gamma = S^{-1} \cdot U' \cdot r
 \end{split}
\end{equation}

With the singular values it can be estimated which of the $\gamma$ have a relevant influence. For this, we choose the largest absolute values of $\gamma$ and examine which variables have the biggest influence on it. This will be the variables that explain most of the variance in the data [\cite{christensen2011plane}].

\subsection{Results}
The V resulting from the singular value decomposition of our matrix F looks the following: 
\\
$
 \begin{bmatrix}
 
   -0.4508 &   0.1918& \dots & 0.7490\\
    0.1380  &  0.5067  &  \dots &   0.1988\\
   -0.2275 &  -0.3458  &  \dots &  -0.0987\\
    0.4385  & -0.1307&   \dots  &  0.3383\\
    0.1087 &   0.5214 &   \dots &  -0.2056\\
    0.1107 &  -0.2836&   \dots  &  0.0101\\
    0.1041  & -0.3292&   \dots  &  0.0162\\
   -0.4243 &   0.0783  &  \dots &  -0.1515\\
    0.2907   & 0.0115 &   \dots  & -0.0567\\
    0.2640  &  0.3001 &  \dots &  -0.2033\\
    0.4051  & -0.1099  &  \dots &   0.4054\\
   -0.0207 &   0.0269  & \dots &  -0.0296

\end{bmatrix}
$ 
\\





While the result of $\gamma = S^{-1} \cdot U' \cdot r$ is:
\\
 $
 \begin{bmatrix}
   -1.2660\\
    0.4385\\
   -0.2391\\
    0.6188\\
    0.0880\\
    0.3699\\
    0.1297\\
   -0.0859\\
    0.6744\\
   -1.0122\\
    0.9504\\
    2.4003\\
\end{bmatrix}
$ 
\\
Of $\gamma$, the last value is by far the one with the highest absolute value. Thus, the last column of V is examined. Here, every row corresponds to a factor. The three variables that have the biggest influence on this gamma are the ones in row one, four and eleven. They correspond to the factors `constant', `body turned' and `long autonomous ride'. Thus, we can assume that those factors have the highest amount of independent impact on takeover time. 

\subsection{Discussion}
In short, what we try to do with the SVD is to find out the most important component of the matrix F and check which factors have the strongest influence on it. The most important factor in the most important component is the `most-most important' factor influencing takeover time. and those are the factors that are assumed to be the most important ones for takeover time. It makes sense that the constant has a high impact on takeover time, since a lot of things that take time have to happen in every takeover, no matter the influencing factors (notice request, motor preparation, move hands to steering wheel, mentally prepare etc.). It is also intuitive that it takes time until the body is turned back towards the road. Interestingly, one of the two factors (besides the constant) represents a motoric transition to driving, while the other represents the mental transition to driving. This perfectly fits with why we did the singular value decomposition: we wanted to find out all factors that have the most impact independent from the others. Thus, it can be assumed that the most crucial points to pay attention to in designing future driving studies are to have one task that is mainly motoric, and one that is mainly cognitive. This is in line with previous research assumptions of nearly all papers that examine takeover time. 
\\
Interestingly, the fourth-most relevant factor is time pressure, which is also a factor that Gold et al. find to have a large impact on resulting takeover time. 
\\
The multiple linear regression in section \ref{regLit} resulted in `traffic' and `Criticality of the situation' having the strongest influence besides the constant. While both are cognitive factors, they have an opposing influence on takeover time: while traffic leads to a longer takeover time, a critical situation leads to a shorter takeover time. Thus, it is still import to differentiate between cognitive factors that have opposing influence. 
\\

\section{Factor Analysis - just notes}
- Ist Faktoranalyse wirklich geeignet, um verschiedene Studien zu vergleichen?Denn in meiner Matrix aus der die Korrelationsmatrix R berechnet wird stehen ja nicht Testergebnisse einer Person (wo der `Unterfaktor' Intelligenz oder sowas ist), sondern was in verschiedenen Studien gemessen wurde. Die Korrelation sagt also nur etwas darüber aus, welche Faktoren oft zusammen gemessen wurden. Wenn ich dann eine Faktoranalyse mache, kommt also raus, welche ‚Wolken‘ von gemeinsam gemessenen Faktoren es gibt. Will ich nicht aber eigentlich wissen, welche ‚Wolken‘ es im Kopf des Menschen gibt? 
-auch in SVD herausheben dass das alles ist was ich mache!
-collinearity not because they belong together but because they are  measured together; thus we get out as much independent info as possible out of all studies, but not what is most important.
- to find out the real underlying variables
- reduce a data set to a manageable size with retaining as much info as possible (e.g. if you have collinearity in a regression: factor analysis combines the variables that are collinear)
-R-matrix is a correlation matrix between all variables
- `By reducing a data set from a group of interrelated variables to a smaller set of factors, factor analysis achieves parsimony by explaining the maximum amount of common variance in a correlation matrix using the smallest number of explanatory constructs'
- In factor analysis we strive to reduce this R-matrix down to its underlying dimensions by looking at which variables seem to cluster together in a meaningful way. This data reduction
is achieved by looking for variables that correlate highly with a group of other variables, but do not correlate with variables outside of that group.
- If each axis on the graph represents a factor, then the variables that go to make up a factor can be plotted according to the extent to which they relate to a given factor.
- the coordinate of a variable along a classification axis is known as factor loading
- the factor loading can be thought of as the pearson correlation between a factor and a variable
- Kapitel 17 weiter anschauen (especially 17.3.3.1 und meine SVD formel besser erklaren); unterschiede SVD PCA Faktoranalysis erklaeren
-Quelle: Andy Field!!!!


%https://stats.stackexchange.com/questions/70899/what-correlation-makes-a-matrix-singular-and-what-are-implications-of-singularit?noredirect=1&lq=1


\section{Regression Analysis of Experimental Data}\label{regBosch}
To compare literature with results of our own study, we also conducted a multiple linear regression with data obtained from our own experiments. 34 subjects conducted six takeovers with each 5 or 15 minutes of non-driving related task in between (depending on their group). They drove on the right lane of a German highway with about 100 km/h. During the `Highly automated driving' mode, a wizard drove the car from the right front (usually co-driver) seat, while the subject sat on the left (usually driver) seat. When a takeover request was issued, the wizard gave control to the subject one second after two levers on the steering wheel were pressed. The subject either 1) did nothing (baseline), 2) listened to an audiobook, 3) searched a figure out of several on the back seat, 4) read a journal or 5) played tetris on a tablet.
 
\subsection{Regression Equation}
For the regression with out own data we included as much as possible of the factors that were also measured in the literature regression. `Criticality of the situation' could not be assessed, because the study took place on a real road. Thus, all situations were uncritical. Therefore, mirror checks and decisions were never required. Takeover time was again measured in seconds. Age was binary for under and above 45 years; Gaze off road was binary; Hand and body position were binary; what was called `traffic' in the literature regression was here measured in whether a car was in front which could be `no car ahead', `appropriate distance to front vehicle' or `short distance to front vehicle'. The length of the ride before takeover was either `long' (15 minutes) or `short' (5 minutes). Mental workload (=Cognitive Distraction) during takeover request was rated by participants on a scale from 0 to 15. Perceived Time Pressure at the takeover request was rated on a scale from 1-3. 
\\
Thus, we postulate the following regression equation:
\begin{equation}
\begin{split}
TakeOverTime = \beta_{0} + \beta_{1} \cdot Gaze Off Road + \beta_{2} \cdot Body Turned Away +\\
 \beta_{3} \cdot Hands Occupied  + \beta_{4} \cdot Cognitive Distraction + \beta_{5} \cdot Much Traffic +\\
\beta_{6} \cdot  Time Pressure + \beta_{7} \cdot Long Autonomous Ride  + \beta_{8} \cdot Elderly Subject  + \\ %%%%%%%%%%%%%%%
\epsilon_{i}
 \end{split}
\end{equation}


\subsection{Checking the Multiple Regression Requirements}
\subsubsection{Linearity of the problem}
For a description, please refer to \ref{req1}. In this case, the diagrams for Cognitive Distraction, time pressure and age look linear, while the ones for Gaze Off Road, Hands Occupied, Body Turned Away, Long Autonomous Ride and traffic are most likely not linear. This should be kept in mind. 

\begin{figure}
    \centering
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Figures/20}
        \caption{Gaze Off Road}
        \label{fig:gull}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Figures/21}
        \caption{Hand position}
        \label{fig:tiger}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Figures/22}
        \caption{Body position}
        \label{fig:mouse}
    \end{subfigure}
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Figures/23}
        \caption{Long autonomous ride}
        \label{fig:gull}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Figures/24}
        \caption{Cognitive Distraction}
        \label{fig:tiger}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
    %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Figures/25}
        \caption{Time pressure}
        \label{fig:mouse}
    \end{subfigure}
        \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Figures/26}
        \caption{Elderly subject}
        \label{fig:gull}
    \end{subfigure}
    ~ %add desired spacing between images, e. g. ~, \quad, \qquad, \hfill etc. 
      %(or a blank line to force the subfigure onto a new line)
    \begin{subfigure}[b]{0.3\textwidth}
        \includegraphics[width=\textwidth]{Figures/27}
        \caption{Traffic}
        \label{fig:tiger}
    \end{subfigure}
    \caption{Partial Regression Diagrams}\label{regdiag2}
\end{figure}


\subsubsection{Linearity of the coefficients (Gauss-Markov-Theorem 1)}
We assume that the postulated model is linear in the coefficients: 

\begin{equation}
\begin{split}
TakeOverTime = \beta_{0} + \beta_{1} \cdot Gaze Off Road + \beta_{2} \cdot Mirror Check Necessary + \\
\beta_{3} \cdot Body Turned Away + \beta_{4} \cdot Hands Occupied  + \beta_{5} \cdot Cognitive Distraction + \\
\beta_{6} \cdot Much Traffic +  \beta_{7} \cdot Decision Necessary + \\
\beta_{8} \cdot  Time Pressure + \beta_{9} \cdot Long Autonomous Ride  + \beta_{10} \cdot Elderly Subject  + \\ 
\epsilon_{i}
 \end{split}
\end{equation}

\subsubsection{Random Sample (Gauss-Markov-Theorem 2)}
See \ref{req3} %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsubsection{Strict Exogeneity (Gauss-Markov-Theorem 3)}
See \ref{req4} and \ref{28}. Even though the data points are higher on the positive side of zero, this is assumed to be evened out by a larger amount of data points smaller than zero. In case there would be an upwards trend, this would imply the existence of an implicit X. This means that a variable that influences takeover time in all experiments is not included in the regression equation. This would lead to biased fitted estimates for the slope and intercept. Another option in case of linear dependency in the plot of residuals against fitted values is an implicit block effect. This can be detected by performing an analysis of covariance. This divides the data into different regression lines. If there is a significant difference between the slopes, the linear relationship between X and Y varies with the value of the blocking factor [\cite{statsrequ}]. For simplicity, we will assume strict exogeneity. 

\begin{figure}
\centering
\includegraphics[width = 0.5\textwidth]{Figures/28}
\decoRule
\caption{Scatter Plot of errors.}
\label{28}
\end{figure}

\subsubsection{Homoscedasticity}
See \ref{req4}. There is no shape in \ref{28} 

\subsubsection{Independence of the Error Term}
See \ref{req5}.  In our case, the Durbin-Watson value is 1.76, which indicates medium correlation between the error terms. 

\subsubsection{Normal Distribution of the Error Term}
The residuals should be normally distributed. This can be checked and confirmed by Fig. \ref{29}. The distribution has a slight skew to the right.

\begin{figure}
\centering
\includegraphics[width = 0.5\textwidth]{Figures/29}
\decoRule
\caption{Histogram of residuals.}
\label{29}
\end{figure}

\subsubsection{No Multicollinearity}

\begin{figure}
\centering
\includegraphics[width = 1\textwidth]{Figures/30}
\decoRule
\caption{Correlations between all variables .}
\label{corr2}
\end{figure}


For explanation, see \ref{multi}. There is one high correlation between `hand position' and `gaze offroad'. This can be seen in Fig. \ref{corr2} . It is the same correlation we also found in the literature regression, and it makes sense because one that does a task with his hands most likely also looks at that task. 

\begin{figure}
\centering
\includegraphics[width = 1\textwidth]{Figures/31}
\decoRule
\caption{Coefficients and multicollinearity measures.}
\label{31}
\end{figure}

As can be seen in Fig. \ref{31}, none of the Tolerance Values (`Toleranz', second-last column under `Kollinearitaetsstatistik') are smaller than 0.1. 

\subsection{Results}\label{resData}
In the same Figure (\ref{31}) it can be seen that besides the constant (`Konstante'), hands occupied (`Handposition') and length of ride before takeover request (`Dauer Nebenaufgabe'), none of the coefficients are significant (sixth column `Sig.'). Nonsignificant variables need to be excluded in the regression equation. Thus, our resulting regression equation is:

\begin{equation}
\begin{split}
TakeOverTime = 1.95 + 1.45 \cdot Hands Occupied - 0.71 \cdot Length Of Ride Before TOR + \epsilon_{i}
 \end{split}
\end{equation}

Other than in the literature regression, here the betas do not correspond to seconds. They can only be compared in relation to each other. Thus, while the biggest influence comes from the constant, also hand occupation has a relevant influence on takeover time. This, again, could be a representative of `motor' readiness, while `length of ride before TOR' could be a representative of the `cognitive' readiness. 
For explanation, see \ref{resLit}. Again, Goodness of fit ($R^2$) is 0.22 - thus, the included variables explain 22\% of the variability in takeover time. A scatter plot of predicted against measured takeover time can be found in Fig. \ref{33}.

\begin{figure}
\centering
\includegraphics[width = 1\textwidth]{Figures/32}
\decoRule
\caption{Goodness of fit and ANOVA.$(R^2) = 0.22$.}
\label{3}
\end{figure}

\begin{figure}
\centering
\includegraphics[width = 0.5\textwidth]{Figures/33}
\decoRule
\caption{Predicted against measured takeover time.}
\label{33}
\end{figure}

\subsection{Discussion and Outlook}

In this regression, the linearity of the problem, the independence of the error term and a correlation between two factors (`hand position' and `gaze off road') are not perfectly given. If requirements of the multiple regression are not met, this decreases the power of the test and thus leads to high p-values. The dependence of the error term is rather small. Because the VIF indicates no multicollinearity, the correlation can most likely be neglected. To reach linearity of the problem, it might be interesting to transform the data. What needs to be kept in mind is that nothing about the regression model says that the explanatory variables actually explain the dependent variable. In addition, variables that `truly' come from the same underlying factor (see \ref{svd}) compete for explanatory power and maybe only one out of several becomes significant. Thus, it might well be that the `general' message of one cognitive influencing factor with smaller impact and one motor influencing factor with larger impact is true.



